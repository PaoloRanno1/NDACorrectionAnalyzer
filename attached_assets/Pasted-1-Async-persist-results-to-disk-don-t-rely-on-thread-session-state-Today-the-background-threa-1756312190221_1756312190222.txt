1) Async: persist results to disk (donâ€™t rely on thread â†’ session_state)

Today the background thread writes big blobs into st.session_state.direct_processing['results']. Streamlit state isnâ€™t thread-safe, so those writes can be lost, and after a rerun the bytes are gone. Persist files to disk keyed by job_id, and only put a tiny results_path into state. Your UI can always reload from disk even after a rerun. 

In direct_tracked_async (5).py

A. Add helpers at the top (imports + job dir):

# ADD near other imports
import json
from pathlib import Path

_BASE_DIR = Path("direct_jobs")
_BASE_DIR.mkdir(exist_ok=True)

def _job_dir(job_id: str) -> Path:
    d = _BASE_DIR / job_id
    d.mkdir(parents=True, exist_ok=True)
    return d


B. After you read the generated files (tracked/clean), write them to disk and store a path (not bytes) in state.
(Place right after you read tracked_bytes / clean_bytes in _run_direct_tracked_pipeline.)

# ADD right after reading tracked_bytes/clean_bytes
jobdir = _job_dir(job_id)
(tracked_path_out := jobdir / "tracked.docx").write_bytes(tracked_bytes)
(clean_path_out   := jobdir / "clean.docx").write_bytes(clean_bytes)
(jobdir / "meta.json").write_text(json.dumps({
    "original_filename": filename,
    "ts": time.time(),
}, indent=2))

# Store only a pointer in state; avoid dumping bytes from background thread
_set_status(
    status='completed', progress=100, message='Direct generation completed!',
    results=None,  # DO NOT pass bytes here
    results_path=str(jobdir),  # <- pointer
)


(You currently set results={...bytes...} here; replace that call with the snippet above. 
)

C. Teach the status UI to read from disk when done.
In render_direct_tracked_status_ui() replace the â€œcompletedâ€ branch to load files if we only have a results_path:

# REPLACE the 'elif dp["status"] == "completed"' block with:
elif dp['status'] == 'completed' and (dp.get('results') or dp.get('results_path')):
    st.success('âœ… Direct tracked changes generation completed!')
    import os

    # Load from disk if needed
    if not dp.get('results') and dp.get('results_path'):
        jobdir = Path(dp['results_path'])
        tracked_bytes = (jobdir / "tracked.docx").read_bytes()
        clean_bytes   = (jobdir / "clean.docx").read_bytes()
        meta = json.loads((jobdir / "meta.json").read_text()) if (jobdir / "meta.json").exists() else {}
        original_filename = meta.get("original_filename", "NDA.docx")
    else:
        # Back-compat (if results dict exists)
        res = dp['results']
        tracked_bytes = res['tracked_changes_content']
        clean_bytes   = res['clean_edited_content']
        original_filename = res.get('original_filename', 'NDA.docx')

    base_name = os.path.splitext(original_filename)[0]
    col1, col2 = st.columns(2)
    with col1:
        st.download_button(
            'ðŸ“„ Download Tracked Changes',
            data=tracked_bytes,
            file_name=f"{base_name}_Tracked_Changes.docx",
            mime='application/vnd.openxmlformats-officedocument.wordprocessingml.document',
            use_container_width=True
        )
    with col2:
        st.download_button(
            'ðŸ“„ Download Clean Version',
            data=clean_bytes,
            file_name=f"{base_name}_Clean_Edited.docx",
            mime='application/vnd.openxmlformats-officedocument.wordprocessingml.document',
            use_container_width=True
        )

    # (Optional) keep your Issues Processed expander below as-is


This removes the race on session state updates from the worker thread and makes results survive reruns. (You already render progress + status from direct_processing; keep that. 
)

2) Sync â€œDirect tracked changesâ€: handle zero-issues properly

When total_issues == 0 you only show a success message and delete tempsâ€”no results savedâ€”so after any rerun thereâ€™s nothing to display/download. Fix: save the original DOCX bytes as both outputs and set the â€œreadyâ€ flag. 

In app (5).py

Find the block:

if total_issues == 0:
    print("[DIRECT] No compliance issues found - NDA is fully compliant!")
    progress_bar.progress(1.0)
    status_text.success("âœ… No compliance issues found! Your NDA is fully compliant.")
    os.unlink(temp_file_path)
    os.unlink(converted_path)
    print("[DIRECT] Temporary files cleaned up")


Replace with:

if total_issues == 0:
    print("[DIRECT] No compliance issues found - NDA is fully compliant!")
    progress_bar.progress(1.0)
    status_text.success("âœ… No compliance issues found! Your NDA is fully compliant.")

    # Persist results so UI always shows downloads
    original_docx_bytes = file_content  # we already have this from the upload
    st.session_state.direct_sync_results = {
        'tracked_docx': original_docx_bytes,  # same as input
        'clean_docx': original_docx_bytes,    # same as input
        'filename': uploaded_file.name,
        'high_priority': [],
        'medium_priority': [],
        'low_priority': [],
        'total_issues': 0,
    }
    st.session_state.direct_results_ready = True
    print("[DIRECT] Zero-issue result stored in session state")

    # Cleanup temps
    os.unlink(temp_file_path)
    os.unlink(converted_path)
    print("[DIRECT] Temporary files cleaned up")


Your download rendering already reads direct_sync_results + direct_results_ready later, so this will reliably show the buttons. 
 

3) Be resilient to Gemini overload/timeouts during analysis (not only cleaning)

Right now you have a nice fallback for the cleaning step (503 â†’ use original findings), but if the analysis call itself (review_chain.analyze_nda(...)) fails, the flow bails out and nothing is shown. Add a small retry + fallback to â€œ0 issuesâ€ so users still get documents. 
 

In app (5).py

Right before:

compliance_report, raw_response = review_chain.analyze_nda(converted_path)


drop in this helper + use it:

# ADD above the call (inside the same try block)
import time
def _retry_analyze(chain, path, retries=2, backoff=2.0):
    last_err = None
    for i in range(retries + 1):
        try:
            return chain.analyze_nda(path)
        except Exception as e:
            msg = str(e)
            if any(k in msg for k in ("503", "UNAVAILABLE", "overloaded", "timed out", "timeout")) and i < retries:
                time.sleep(backoff * (i + 1))
                continue
            last_err = e
            break
    # Fallback: no findings
    return ({'high_priority': [], 'medium_priority': [], 'low_priority': []}, None)

# REPLACE the direct call with:
compliance_report, raw_response = _retry_analyze(review_chain, converted_path)


This ensures you donâ€™t end the run with an error screen and no artifacts if Gemini is moody. 

4) (Optional) Make async UI robust to reruns

Keep a tiny â€œrefresh statusâ€ button (you already have one) and avoid relying on dp['results'] entirelyâ€”use the results_path approach above. Your current UI already builds a nice status box with a Refresh + Cancel; keep that UX. 

Why this fixes the flakiness

Thread safety: the worker no longer pushes large blobs into Streamlit state (which can be dropped across reruns). It writes files to disk and leaves a breadcrumb (results_path), so the UI can always reload them. 

Zero-issues path: you now persist a valid result, so reruns still show download buttons. 

Gemini wobble: retries/fallback ensure the pipeline still produces documents even when the API returns 503/timeouts. 

If you want, I can also hand you a tiny â€œdirect_jobs cleanerâ€ snippet or wire the async and sync flows to a shared â€œresults loader,â€ but the three changes above are the big wins.